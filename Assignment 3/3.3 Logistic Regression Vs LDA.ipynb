{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.3: Logistic Regression vs. LDA \n",
    "\n",
    "In this part, you will explore the choice between Logistic Regression and Linear Discrimnant Analysis for a given dataset. In addition, you will learn how to visualize the probabilistic output of Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization function of Logistic Regression\n",
    "\n",
    "In your lecture slides, the following cost function for Logistic Regression has been introduced and needs to get minimized to determine the optimal choice of the parameters $\\mathbf{w}$:\n",
    "\n",
    "\\begin{equation}\n",
    "J_{1}(w) = \\sum_{i=1}^{N_{samples}} \\ln(\\exp(-\\mathbf{y_{i}}(\\mathbf{w}^{\\rm T}\\mathbf{x_{i}}))+1)\n",
    "\\end{equation}\n",
    "\n",
    "### Q3.3.1 Assuming linear separability of a given data set, what can happen to the weights $\\mathbf w$ when minimizing the cost function $J_{1}(\\mathbf{w})$? Why is this a problem?\n",
    "\n",
    "__Hint:__  Assume it exists a weight vector $\\mathbf{\\tilde w}$ such that $\\mathbf{y_{i}}(\\mathbf{\\tilde w}^{\\rm T}\\mathbf{x_{i}}) > 0$ for all sample points $i$. The class labels are defined as $y_{i}=\\{-1,1\\}$.\n",
    "\n",
    "\n",
    "---------------------------------------------------\n",
    "\n",
    "A more stable estimate for $\\mathbf{w}$ can be achieved when using the concept of __Regularization__. Adding an additional term $J_{2}(\\mathbf{w}) = \\frac{1}{2}\\mathbf{w}^{\\rm T}\\mathbf{w}$ to the cost function modifies the overall cost function $J(\\mathbf{w},C)$ to: \n",
    "\n",
    "\\begin{equation}\n",
    "J(w,C) = J_{1}(\\mathbf{w},C) + J_{2}(\\mathbf{w}) = C\\cdot \\sum_{i=1}^{N_{samples}} \\ln(\\exp(-\\mathbf{y}(\\mathbf{w}^{\\rm T}\\mathbf{X}))+1) + \\frac{1}{2}\\mathbf{w}^{\\rm T}\\mathbf{w}\n",
    "\\end{equation}\n",
    "\n",
    "Note, that the so-called regularization parameter $C$ is a fixed value and determines the relation of the second cost function $J_{2}(w)$ with respect to the first cost function $J_{1}(w)$.\n",
    "\n",
    "### Q3.3.2 Which role has the additional cost function $J_{2}(w)$?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate and visualize the output of a Logistic Regression model\n",
    "\n",
    "__Tasks:__ \n",
    "1. Visualize the given data set using the class labels. \n",
    "2. Lookup the scikit-learn toolbox implementation of a Logistic Regression model. Then train a model on the given training data. Please make sure that you use an \"L2-penalty\" for regularization and set your regularization parameter to C = 1.\n",
    "3. Use your trained Logistic Regression model to predict the class labels of the test data and report on the classification performance.\n",
    "4. Inspect the given function \"plot_probabilityMap\" and use it to visualize the probability map of class \"1\" of your trained Logistic Regression model using the test data set as an input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds_trainData = 'data/spread_train.data'\n",
    "ds_trainLabels ='data/spread_train.labels'\n",
    "ds_testData = 'data/spread_test.data'\n",
    "ds_testLabels ='data/spread_test.labels'\n",
    "\n",
    "# load training data\n",
    "X_train = np.loadtxt(ds_trainData)\n",
    "y_train = np.loadtxt(ds_trainLabels)\n",
    "\n",
    "# load test data\n",
    "X_test = np.loadtxt(ds_testData)\n",
    "y_test = np.loadtxt(ds_testLabels)\n",
    "\n",
    "# (1.)-> Scatter-plot of your 2D-training data using the class labels of the training data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2.) Train a logistic regression model\n",
    "\n",
    "\n",
    "# (3.) Predict the class labels on the test data and return a classfication rate (0-1-Loss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_probabilityMap(model, X, y, y_pred, plot_means, class_index):\n",
    "    '''visualizes the output of the applied models with a\n",
    "    probability map\n",
    "    INPUT: \n",
    "    'model'-> scikit-learn like instance of classifier models with built-in method 'predict_proba' available\n",
    "    'X' -> data matrix of size NxD\n",
    "    'y' -> true class labels\n",
    "    'y_pred' -> predicted class labels\n",
    "    'plot_means' -> only for LDA: adding class means to the plot\n",
    "    'class_index' -> probability map for class, either '0' or '1'\n",
    "    \n",
    "    OUTPUT:\n",
    "    splot: figure handle of the plot\n",
    "    '''\n",
    "    splot = plt.figure()\n",
    "    \n",
    "    tp = (y == y_pred)  # True Positive\n",
    "    tp0, tp1 = tp[y == 0], tp[y == 1]\n",
    "    X0, X1 = X[y == 0], X[y == 1]\n",
    "    X0_tp, X0_fp = X0[tp0], X0[~tp0]\n",
    "    X1_tp, X1_fp = X1[tp1], X1[~tp1]\n",
    "\n",
    "    # class 0: true positives vs. false positives\n",
    "    plt.plot(X0_tp[:, 0], X0_tp[:, 1], 'o', color='red')\n",
    "    plt.plot(X0_fp[:, 0], X0_fp[:, 1], 'o', color='salmon')  # dark red\n",
    "\n",
    "    # class 1:\n",
    "    plt.plot(X1_tp[:, 0], X1_tp[:, 1], 'o', color='blue')\n",
    "    plt.plot(X1_fp[:, 0], X1_fp[:, 1], 'o', color='lightskyblue')  # dark blue\n",
    "\n",
    "    # class 0 and 1 : areas\n",
    "    nx, ny = 200, 100\n",
    "    x_min, x_max = plt.xlim()\n",
    "    y_min, y_max = plt.ylim()\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, nx),\n",
    "                         np.linspace(y_min, y_max, ny))\n",
    "    Z = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z[:, class_index].reshape(xx.shape)\n",
    "    plt.pcolormesh(xx, yy, Z, cmap='YlOrBr')\n",
    "    # colorbar assigning the probabilities\n",
    "    plt.colorbar()\n",
    "        \n",
    "    # plot the contour of the 50 probability\n",
    "    plt.contour(xx, yy, Z, [0.5], linewidths=2., colors='k')\n",
    "\n",
    "    # plot class means (only makes sense for LDA)\n",
    "    if plot_means: \n",
    "        plt.plot(model.means_[0][0], model.means_[0][1],\n",
    "                 'o', color='black', markersize=10)\n",
    "        plt.plot(model.means_[1][0], model.means_[1][1],\n",
    "                 'o', color='black', markersize=10)\n",
    "\n",
    "    return splot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (4.): Visualize the Logistic Regression Output\n",
    "# Hint: set the flag \"plot_means\" to False as the Logistic Regression Model does not depend on the class means\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with performance of the LDA \n",
    "\n",
    "For comparison, you now need to train and evaluate an LDA model on the same data set and evaluate its performance.\n",
    "\n",
    "__Tasks:__\n",
    "1. Lookup the scikit-learn toolbox implementation for an LDA model. Then train a model on the given training data. Please specify your solver to \"eigen\".\n",
    "2. Use your trained LDA model to predict the class labels of the test data and report on the classification performance.\n",
    "3. Apply the given function \"plot_probabilityMap\" to visualize the probability map of class \"1\" of your trained LDA model taking the test data set as an input. Use the input parameter of the function to visualize the class means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Train an LDS model\n",
    "\n",
    "\n",
    "# (2) Predict the class labels on the test data and return a classfication rate (0-1-Loss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3): Visualize the LDA output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3.3.3) Compare the performance of the Logistic Regression model with the LDA for the given dataset. Do you observe a severe difference in the performance of both models? Can you explain the reason?\n",
    "__Hint__: You can compare the two decision boundaries.\n",
    "\n",
    "\n",
    "\n",
    "#### Q3.3.4) How can you estimate a probability from an LDA classifier? Remember that this is not a probabilistic method by default.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
