{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.2: Linear Regression with Gradient Descent\n",
    "\n",
    "In exercise 3.1 you have tackled the Ordinary Least Squares Linear Regression by calculating the analytical solution to estimate the coefficients $\\hat{\\boldsymbol w}$. In this assignment you will solve the same optimization problem using a standard numerical optimization method called Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from sklearn import datasets, linear_model\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.1 Getting started with the data set\n",
    "\n",
    "__Tasks:__ \n",
    "1. Visualize the given data X and y which will be used to fit a linear regression model. How many feature dimensions are contained?\n",
    "2. Split the given data X and y into train (first 100 data points) and test set (last 200 data points).\n",
    "\n",
    "__Hint:__ \n",
    "In this assignment, the augmented notation for the data $\\mathbf{X}$ is used such that the estimated coefficients $\\hat{\\boldsymbol w}$ fit the augmented model $f(\\mathbf X) = \\mathbf X \\hat{\\boldsymbol w}$\n",
    "\n",
    "__Answer:__\n",
    "The data has only one feature dimension, the second column consists only of ones (due to the augmented notation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/GD_dataset1.data'\n",
    "label_dir = 'data/GD_dataset1.labels'\n",
    "\n",
    "X = np.loadtxt(data_dir)\n",
    "y = np.loadtxt(label_dir)\n",
    "\n",
    "# (1.) -> check the dimensionality of the given data\n",
    "\n",
    "\n",
    "# (1.) -> plot the given data (D=1)\n",
    "\n",
    "\n",
    "# (2.) -> split the data in train/test set\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.2 Explore the behaviour of the cost function\n",
    "\n",
    "In order to compute the coefficients $\\hat{\\boldsymbol w}$ for the OLS Linear Regression, a cost function $J(\\mathbf{w})$ has been introduced in the lecture.\n",
    "\n",
    "__Tasks:__ \n",
    "1. Look up the definition of the cost function for ordinary least-squares (OLS) linear regression and implement it. The \"cost_function()\" should return a single scalar which is the evaluation of the cost function $J(\\mathbf{w})$.\n",
    "2. Evaluate your implemented cost function $J({w}_{1})$ by varying the parameter $w_{1}\\in[-500, 600]$ (you can set the bias term $w_{0}=0$) using the training data $\\mathbf{X}_{train}$ and $\\mathbf{y}_{train}$. Plot the resulting cost function $J({w}_{1})$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1.) -> Implementation of the cost function for linear regression\n",
    "\n",
    "def cost_function(X, y, w):\n",
    "    \"\"\"cost_function(X, y, w) compute the MSE cost using w as the\n",
    "    coefficients for linear regression to fit the data points in X and y\n",
    "    \n",
    "    Input:\n",
    "    X: array with data matrix X\n",
    "    y: observed output vector\n",
    "    w: regression weights (compact form)\n",
    "\n",
    "    Return: \n",
    "    J: scalar cost\n",
    "\n",
    "    \"\"\"\n",
    "    ## number of training examples\n",
    "    N_sam = X.shape[0]\n",
    "     \n",
    "    ## Calculate the MSE cost with the given parameters\n",
    "    J = ...\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2.) -> Evaluation and plot the cost function for the given training data\n",
    "w_range = np.arange(-500, 600, 20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.3 Optimize OLS Linear Regression with Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 3.3.2. you have already explored the full landscape of the cost function. If we now assume that a single evaluation of J is (e.g. computationally) expensive, then you want to use a strategy with as few evaluations as possible to find $\\boldsymbol w^{*}$ with the minimum cost $J({\\boldsymbol w^{*}})$. To do so, we will now use an iterative approach to find $J(\\boldsymbol w^{*})$ of the linear regression coefficients $\\boldsymbol w$. The convergence of the utilized gradient descent algorithm shall be investigated with respect to its parameters (learning rate $\\eta$, number of iterations $N_{iter}$) and its estimated coefficients $\\hat{\\boldsymbol w}$ shall be compared to those the analytical solution of the OLS linear regression.\n",
    "\n",
    "__Tasks:__\n",
    "\n",
    "1. Lookup the scikit-learn toolbox implementation of the analytical OLS linear regression model or use your code of Assignment 3.1. Train the model on the given training data $\\mathbf{X}_{train}$ and $\\mathbf{y}_{train}$ and print the estimated coefficients $\\hat{\\boldsymbol w}$ of the analytical solution.\n",
    "2. Inspect the given function \"gradient_descent()\" (below). Use it to optimize the cost function by varying the number of iteration steps as $N_{iter}\\in\\{2,5,10,20,40\\}$. Keep the learning rate fixed at $\\eta=0.1$ and use the given starting value $\\boldsymbol w_{start}=(-400,0)^{T}$.\n",
    "3. Create a scatter plot of the test data $\\mathbf{X}_{test}$ and $\\mathbf{y}_{test}$ and add the corresponding linear regression fit for each parameter configuration $N_{iter}$ to the plot. In addition, add the linear regression fit that you gained from the analytical solution in (1.).\n",
    "4. Plot the cost function of part 3.3.2 again and add the five evalutated cost function values gained by varying  $N_{iter}$.\n",
    "5. Repeat the steps (2.)-(4.) by varying the learning rate $\\eta \\in \\{10^{-4}, 10^{-2}, 0.02, 1\\}$, but fix the number of iterations $N_{iter}$ to 5. Use the same $\\boldsymbol w_{start}$ as in (2.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1.) -> Analytic solution for OLS Linear Regression (with scikit-learn implementation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, eta, N_iterations, w_start=np.array([0,0])):\n",
    "    \"\"\"gradient_descent() runs batch gradient descent optimization \n",
    "    to fit a regression model y_est = beta * x\"\n",
    "    \n",
    "    Input:\n",
    "    X: array with data matrix X\n",
    "    y: vector with observed output\n",
    "    eta: learning rate of gradient descent\n",
    "    N_iterations: total number of iterations for the optimization\n",
    "    w_start: initial configuration of coefficients\n",
    "        \n",
    "    Return:  \n",
    "    beta_opt: array with optimal parameter set\n",
    "    cost_history: array of size N_iterations with history of cost function\n",
    "    \"\"\"\n",
    "    cost_history = [0] * N_iterations\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # initialize the coefficients\n",
    "    w=w_start\n",
    "    \n",
    "    for iteration in range(N_iterations):\n",
    "        hypothesis = X.dot(w)\n",
    "        loss = hypothesis-y\n",
    "        gradient = X.T.dot(loss)/m\n",
    "        w = w - eta*gradient\n",
    "        cost = cost_function(X, y, w)\n",
    "        cost_history[iteration] = cost\n",
    "    \n",
    "    w_opt = w\n",
    "    return w_opt, cost_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variation of the total number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for batch gradient descent\n",
    "N_iterations = [2,5,10,20,40]\n",
    "eta = 0.1\n",
    "w_start = np.array([-400,0])\n",
    "\n",
    "# (2.) - (4.) -> Variation of the number of iterations \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.2.1: What can you observe from the two plots for an increased number of iterations $N_{iter}$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.2.2: Assume you apply your best linear regression model $\\hat{\\mathbf{w}}$ to your *test* data and compare the test error to your training error. What do you expect and why? What is your expectation if you would do the same with the analytical solution of the OLS linear regression model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variation of the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for batch gradient descent\n",
    "N_iterations = 5\n",
    "eta = [1e-4, 1e-2, 0.02,1]\n",
    "w_start = np.array([-400,0])\n",
    "\n",
    "# (5.) -> Variation of the learning rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.2.3: What happens if you choose a learning rate $\\eta >> 1$ ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.3 Pitfalls of gradient descent\n",
    "\n",
    "__Bonus:__ The use of iterative optimization methods (such as gradient descent) is not restricted to linear models, and for some problems an analytical solution may not even exist (or has not been found yet). For this reason, many complex Machine Learning models are trained using iterative optimization. \n",
    "However, iterative optimization of more complex problems comes at a price... Let's take a look at such situations by choosing a different model class (sine instead of linear functions)!\n",
    "\n",
    "__Tasks:__ \n",
    "1. Visualize the given data set. Note that the second dimension of $\\mathbf{X}$ contains ones to make use of the augmented notation.\n",
    "2. Modify your MSE cost function of section 3.3.2 such that you can fit a model $f(\\mathbf{x})=\\sin(\\mathbf{w}^T\\mathbf{x})$.  \n",
    "3. Set $w_{1}=1$ and evaluate the cost function $J(w_{0})$ for values $w_{0}\\in[-4\\pi,4\\pi]$. Visualize your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/GD_dataset2.data'\n",
    "label_dir = 'data/GD_dataset2.labels'\n",
    "\n",
    "X = np.loadtxt(data_dir)\n",
    "y = np.loadtxt(label_dir)\n",
    "\n",
    "# (1.) -> Visualize the given data set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2.) -> Modified cost function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3.) -> Evaluate the cost function for different values of w_0\n",
    "w_range = np.arange(-4*np.pi, 4*np.pi, .05)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.2.4: What happens if you now apply the iterative gradient descent approach?\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
