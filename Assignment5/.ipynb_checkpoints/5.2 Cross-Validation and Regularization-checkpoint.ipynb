{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 5.2 Cross-Validation and Regularization\n",
    " \n",
    " In practical applications the ground truth function will not be given, yet we still have to find a way to balance bias and variance of our models. In order to judge these statistics, one can either keep a disjunct validation set or apply cross-validation if training data is scarce. It is also good practice to keep an additional test set, that is untouched for optimization, in order to judge future performance. The most common method against overfitting is keeping the L2-Norm of the weights as small as possible (also known as Ridge Regression for linear models). This exercise will let you explore the relationship between these concepts.\n",
    "\n",
    "**Task a) Cross-validation and Regularization.**\n",
    "  * Implement Ridge Regression for polynomial models. Don't use sklearn here!\n",
    "  * Implement 3-fold cross-validation and calculate training and validation errors.\n",
    "  \n",
    "**Q 5.3.1: What is the effect of the regularization weight on the different evaluation statistics (training, cross-validation and test-error)?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%run '5.1 Bias-Variance Analysis.ipynb'\n",
    "# We silently run your solution to the first exercise to have\n",
    "# the previously defined functions and data available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_folds = 3\n",
    "n_samples = 1\n",
    "\n",
    "# We draw the points - as before - from a sine curve\n",
    "X_cv = np.array([np.random.uniform(0.1, 0.9, size=(n_samples, 1)) * (-1)**i for i in range(n_folds)])\n",
    "y_cv = np.array([target(X_cv[i]) for i in range(n_folds)])\n",
    "\n",
    "X_test = np.array([-0.97, -0.4, 0.7, 0.97]).reshape((-1, 1))\n",
    "y_test = target(X_test)\n",
    "X_cv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-b33216159258>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0mregularization_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0mavg_training_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_validation_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_test_error\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0mcross_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregularization_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;31m# Show the statistics for this model class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-b33216159258>\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(regularization_weight)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_cv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# TODO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_cv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# TODO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mverify_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mX_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_cv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mun_mask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# TODO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-54333951d5f9>\u001b[0m in \u001b[0;36mverify_shapes\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mverify_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "try: import seaborn as sns\n",
    "except ImportError: pass\n",
    "\n",
    "# Calculate the root mean square error\n",
    "def rmse(y, y_predicted):\n",
    "    return np.sqrt(np.mean(0.5 * (y - y_predicted)**2))\n",
    "\n",
    "\n",
    "# Ridge regression is an extension of linear least squares.\n",
    "# How do you have to adjust the linear fitting function for this?\n",
    "# Remember to augment the dataset for polynomial features as before.\n",
    "def fit_ridge_regression(X, y, regularization_weight):\n",
    "    verify_shapes(X, y)\n",
    "    # TODO(Task a):\n",
    "    bias_col = np.ones((X.shape[0], 1))\n",
    "    phi = np.hstack((bias_col, X))\n",
    "    diag_mat = np.matmul(phi.T, phi) + regularization_weight*np.identity(X.shape[1])\n",
    "    weights = np.matmul(np.matmul(np.linalg.pinv(diag_mat), phi.T), y)\n",
    "    return weights\n",
    "#     pass\n",
    "\n",
    "# We can use the same method to predict with a ridge regression\n",
    "# model as in exercise 5.1.\n",
    "\n",
    "# Bookkeeping\n",
    "training_errors = np.zeros((n_folds))\n",
    "validation_errors = np.zeros((n_folds))\n",
    "test_errors = np.zeros((n_folds))\n",
    "models = np.zeros((n_folds, poly_degree + 1, 1))\n",
    "\n",
    "# Here we define our cross-validation function specialized for\n",
    "# our ridge regression models. The regularization weight is\n",
    "# the only hyperparameter.\n",
    "def cross_validate(regularization_weight):\n",
    "    \n",
    "    # Loop over the validation folds\n",
    "    for i in range(n_folds):\n",
    "        \n",
    "        # Split the data into training and validation set\n",
    "        # You can use the mask to select the correct training folds\n",
    "        # X_cv[mask] will give you all folds except for the i-th one\n",
    "        mask = np.arange(n_folds) != i\n",
    "        un_mask = np.arange(n_folds) == i\n",
    "        \n",
    "        # Remember the corret shape (n_samples, n_features)\n",
    "        X_train = X_cv[mask] # TODO\n",
    "        y_train = y_cv[mask] # TODO\n",
    "        print(f'X_train shape: {X_train.shape}, y_train shape: {y_train.shape}')\n",
    "        verify_shapes(X_train, y_train)\n",
    "    \n",
    "        X_valid = X_cv[un_mask] # TODO\n",
    "        y_valid = y_cv[un_mask] # TODO\n",
    "        print(f'X_valid shape: {X_valid.shape}, y_valid shape: {y_valid.shape}')\n",
    "        verify_shapes(X_valid, y_valid)\n",
    "    \n",
    "        # Train a model and determine the training and validation error\n",
    "        # Luckily we wrote a function for this\n",
    "        models[i] = fit_ridge_regression(X_train, y_train,\n",
    "                                         regularization_weight) # TODO\n",
    "        \n",
    "        # Calculate the training, validation and test error\n",
    "        # for the model trained in this fold\n",
    "        # The models can be evaluated with the predict_polynomial function\n",
    "        # from the first part. We already wrote the rmse error function\n",
    "        # for comparing it to the ground truth\n",
    "        y_pred_train = predict_linear(X_train, models[i])\n",
    "        y_pred_val = predict_linear(X_valid, models[i])\n",
    "        y_pred_test = predict_linear(X_test, models[i])\n",
    "        training_errors[i] = rmse(y_train, y_pred_train) # TODO\n",
    "        validation_errors[i] = rmse(y_valid, y_pred_val) # TODO\n",
    "        test_errors[i] = rmse(y_test, y_pred_test) # TODO\n",
    "            \n",
    "    # Calculate the statistics for the whole procedure\n",
    "    # This reduces to taking the mean over the folds\n",
    "    # The cross-validation error is the measure we're after!\n",
    "    avg_training_error = np.mean(training_errors, axis=0) # TODO\n",
    "    cross_validation_error = np.mean(validation_errors, axis=0) # TODO\n",
    "    avg_test_error = np.mean(test_errors, axis=0) # TODO\n",
    "\n",
    "    return avg_training_error, cross_validation_error, avg_test_error\n",
    "    \n",
    "# TODO: Set the strength of the regularisation here\n",
    "regularization_weight = 0.0\n",
    "avg_training_error, cross_validation_error, avg_test_error = \\\n",
    "    cross_validate(regularization_weight)\n",
    "\n",
    "# Show the statistics for this model class\n",
    "print('Regularization weight: {:.1e}'.format(regularization_weight))\n",
    "print('Average training error: {:.3f}'.format(avg_training_error))\n",
    "print('Cross-validation error: {:.3f}'.format(cross_validation_error))\n",
    "print('Average test error: {:.3f}'.format(avg_test_error))\n",
    "\n",
    "# Plotting: For 3 folds, we can still look at each model and how\n",
    "# well it fits the different data sets.\n",
    "fix, axes = plt.subplots(n_folds, 1, figsize=(12, 16))\n",
    "X_eval = np.linspace(-1, 1, 200).reshape((-1, 1))\n",
    "y_eval = np.sin(np.pi * X_eval)\n",
    "for i, axis in enumerate(axes):\n",
    "    axis.set_title('Fold {:d}'.format(i))\n",
    "    axis.plot(X_eval, np.sin(np.pi * X_eval), label='Target function', alpha=0.5)\n",
    "    if not np.any(np.isnan(models[i])):\n",
    "        axis.plot(X_eval, eval_polynomial(X_eval, models[i]),\n",
    "                  label='Fitted model', c='m')\n",
    "    mask = np.arange(n_folds) != i\n",
    "    axis.scatter(X_cv[mask].flatten(), y_cv[mask].flatten(),\n",
    "                 label='Training samples', s=30, c='k')\n",
    "    axis.scatter(X_cv[i].flatten(), y_cv[i].flatten(),\n",
    "                 label='Validation samples', s=40, c='g')\n",
    "    axis.scatter(X_test.flatten(), y_test.flatten(),\n",
    "                 label='Test samples', s=50, c='r')\n",
    "    axis.set_xlim((-1.1, 1.1))\n",
    "    axis.set_ylim((-1.5, 1.5))\n",
    "    axis.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
