{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting\n",
    "\n",
    "In this notebook, you will use [scikit-learn's regression trees](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) and implement the gradient boosting algorithm for least squares regression. In particular, you will:\n",
    "\n",
    "1. Implement gradient boosting with shrinkage and evaluate its performance on the boston housing data set.\n",
    "2. Investigate the effect of number of iterations and tree depth.\n",
    "3. Plot the additive structure of gradient boosting.\n",
    "\n",
    "The exercise is mostly based on the lecture and the following book:\n",
    "\n",
    "T. Hastie, R. Tibshirani, and J. Friedman: [*The Elements for Statistical Learning*](http://statweb.stanford.edu/~tibs/ElemStatLearn/), 2001\n",
    "\n",
    "As usual, some setup first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.cross_validation\n",
    "import sklearn.datasets\n",
    "import sklearn.dummy\n",
    "import sklearn.tree\n",
    "import sklearn.base\n",
    "import sklearn.metrics\n",
    "import sklearn.ensemble\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the boston housing dataset\n",
    "def create_dataset():\n",
    "    \n",
    "    boston = sklearn.datasets.load_boston()\n",
    "    X = boston.data\n",
    "    y = boston.target\n",
    "    # y += np.random.randn(y.shape[0]) * 3\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        sklearn.cross_validation.train_test_split(X, y, random_state=1)\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBoosting(object):\n",
    "    \n",
    "    def __init__(self, iterations):\n",
    "        # Some global variables\n",
    "        self.iterations = iterations\n",
    "        self.classifiers = list()\n",
    "        self.weak_learner = sklearn.tree.DecisionTreeRegressor(\n",
    "            max_depth=1)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # YOUR TURN\n",
    "        # 1. Initialize f_0\n",
    "        # hint: Scikit-learn offers dummy predictors \n",
    "        # (e.g. DummyRegressor), which return constant \n",
    "        # predictions according to a simple, user-defined rule\n",
    "        \n",
    "        # 2. For m = 1 to m\n",
    "        \n",
    "        # 2.a) compute pseudo residuals\n",
    "        # 2.b) fit regression tree to the residuals\n",
    "        # 2.c) update terminal region: not necessary for \n",
    "        #      squared loss\n",
    "        # 2.d) update f_of_x\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # YOUR TURN\n",
    "        pred = np.zeros(X.shape[0])\n",
    "        return pred\n",
    "    \n",
    "    def staged_predict(self, X):\n",
    "        # returns predictions for X for all iterations \n",
    "        # this is for convenience to simplify computing\n",
    "        # train/test error for all models\n",
    "\n",
    "        staged_predictions = []\n",
    "        # YOUR TURN\n",
    "            \n",
    "        return staged_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your implementation\n",
    "Now, you have a working implementation of the gradient boosting algorithm. \n",
    "\n",
    "To test your algorithm for correctness you can compare to [sklearn's gradient boosting implementation](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html) which should yield the same output as your implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell to test whether your implementation is correct\n",
    "\n",
    "gbm = GradientBoosting(iterations=400)\n",
    "sk_gbm = sklearn.ensemble.GradientBoostingRegressor(\n",
    "    n_estimators=400, learning_rate=1.0, max_depth=1)\n",
    "\n",
    "# Get data\n",
    "X_train, y_train, X_test, y_test = create_dataset()\n",
    "\n",
    "# Fit model\n",
    "gbm.fit(X_train, y_train)\n",
    "sk_gbm.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "pred = gbm.predict(X_test)\n",
    "pred_sk = sk_gbm.predict(X_test)\n",
    "\n",
    "print(\"Difference: %d\" % np.sum([pred - pred_sk]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Add shrinkage\n",
    "\n",
    "The scikit-learn implementation offers a parameter 'learning_rate' which implements skrinkage, such that the contribution of each model is shrinked. Changing line (d) in the Pseudocode (see Algorithm 10.3 *Gradient Tree Boosting Algorithm*) from \n",
    "\n",
    "$f_m(x) = f_{m-1}(x) + \\sum_{j=1}^{J_m} \\gamma_{jm} {\\bf I} (x \\in R_{jm})$\n",
    "\n",
    "to\n",
    "\n",
    "$f_m(x) = f_{m-1}(x) + \\nu \\sum_{j=1}^{J_m} \\gamma_{jm} {\\bf I} (x \\in R_{jm})$\n",
    "\n",
    "will do this. Add a learning rate to your implementation of Gradient Boosting. Therefore you should add an additional parameter 'learning_rate', which you will need in both methods, *predict* and *fit*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playground\n",
    "\n",
    "Next, you can now play around with AdaBoost's hyperparameters. Here are some suggestions to try:\n",
    "\n",
    "* Investigate the number of iterations that are needed to start overfitting.\n",
    "* Add some noise on the targets and try again\n",
    "* Investigate the effect of the tree depth\n",
    "* Investigate effect of shrinkage\n",
    "\n",
    "But first you should implement a plotting function to show train/test error over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_test_error(model, X_train, y_train, X_test, y_test):\n",
    "    ''' Shows a plot with train/test error over iterations\n",
    "\n",
    "    model (GBM): trained GBM algorithm\n",
    "    X_train (array): [#samples, #features]\n",
    "    y_train (array): [#samples, ]\n",
    "    X_test (array): [#samples, #features]\n",
    "    y_test (array): [#samples, ]\n",
    "    '''\n",
    "    # Loop over all predictions and compute the mean squared error\n",
    "\n",
    "    y_pred_train = model.staged_predict(X_train)\n",
    "    training_errors = [sklearn.metrics.mean_squared_error(y_train, y_pred) \n",
    "                       for y_pred in y_pred_train]\n",
    "    \n",
    "    y_pred_test = model.staged_predict(X_test)\n",
    "    test_errors = [sklearn.metrics.mean_squared_error(y_test, y_pred) \n",
    "                   for y_pred in y_pred_test]\n",
    "    \n",
    "    # Plot results\n",
    "    x_values = np.arange(1, len(training_errors) + 1)\n",
    "\n",
    "    plt.plot(x_values, training_errors, label=\"training\")\n",
    "    plt.plot(x_values, test_errors, label=\"test\")\n",
    "    plt.xlabel(\"iterations\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.legend(loc=\"best\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TURN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot individual models\n",
    "Finally, it's time to investigate the additive nature of gradient boosting. For this, we plot the prediction of the gradient boosting model for a sine curve after different amounts of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's some training data:\n",
    "np.random.seed(1)\n",
    "num_data_points = 200\n",
    "lower_bound = -10\n",
    "upper_bound = 10\n",
    "X = np.random.uniform(lower_bound, upper_bound, num_data_points)\n",
    "X = X.reshape((-1, 1))\n",
    "y = (np.sin(X) + np.random.randn(num_data_points).reshape((-1, 1)) * 0.1)\n",
    "y = y.ravel()\n",
    "\n",
    "true_function_X = np.arange(lower_bound, upper_bound, 0.01)\n",
    "true_function_X = true_function_X.reshape((-1, 1))\n",
    "\n",
    "plt.figure(num=None, figsize=(10, 6), dpi=80, facecolor='w', \n",
    "           edgecolor='k')\n",
    "plt.scatter(X.ravel(), y, label=\"training data\")\n",
    "plt.plot(true_function_X, np.sin(true_function_X), \n",
    "         label='true function')\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.ylim([-1, 1])\n",
    "plt.xlim([lower_bound, upper_bound])\n",
    "\n",
    "# YOUR TURN\n",
    "# Plot a line for each model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
