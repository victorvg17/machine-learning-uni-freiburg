{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Trees\n",
    "\n",
    "In this notebook, you will implement a simple regression tree for a one dimensional problem. Using your code, you will explore some basic features of trees and their dependence on their hyperparameters.\n",
    "Before we start, just some setup for the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first building block of your regression tree implementation is the loss function. Implement the residual sum of squares loss discussed in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rss_loss(t_left, t_right):\n",
    "    \"\"\"Computes the residual sum of squares (RSS) for the two target value vectors t_left and t_right.\n",
    "    \n",
    "    To keep this assignment simple, use the mean of the data as the prediction. Based on that,\n",
    "    this function return the sum of both RSS for t_left and t_right\n",
    "    \"\"\"\n",
    "    # Insert code here!\n",
    "\n",
    "    # compute predictive mean of both nodes\n",
    "    \n",
    "    # compute sum of rss in left and right child\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next ingredient for your tree is the split function. It takes the current x and t values, and the min_leaf parameter to compute the split with the minimal loss. It returns the index where to split the two arrays and the split value such that every datapoint with x <= x_split falls into the left child, and every observation with x>x_split into the right one. Note that every x value between the largest in the left and the smallest in the right dataset is valid choices. Commonly, the midpoint is chosen, but you can also pick any random point in the interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split(x , t, min_leaf):\n",
    "    \"\"\"Computes the optimal split for the data set represented by x and t. \"\"\"\n",
    "    \n",
    "    # Insert code below!\n",
    "    \n",
    "    # Loop over all potential partitionings and compute the corresponding loss\n",
    "        \n",
    "    # find the split with minimal loss\n",
    "        \n",
    "    # find simple midpoint\n",
    "        \n",
    "    # for slightly randomized trees, one could choose a random split point here.\n",
    "        \n",
    "    #return best index and split value for the optimal split\n",
    "    return best_index, split_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With everything in place, you can now implement a simple Tree class. The __init__ function just sets up the object. The actual splitting of the data is done in the fit method which should call itself recursively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Tree(object):\n",
    "    \"\"\"A simple regression tree for 1 dimensional functions\"\"\"\n",
    "    def __init__ (self, min_leaf=1, max_depth=10):\n",
    "        # hyperparameters\n",
    "        self.min_leaf=min_leaf\n",
    "        self.max_depth=max_depth\n",
    "        # 'pointers' to the two child nodes\n",
    "        self.subtrees = None\n",
    "        # if this subtree contains no children, this is the prediction of the leaf\n",
    "        self.prediction = None\n",
    "        # if there are two descendents, this is the split value to decide where a given datapoint falls\n",
    "        self.split_value = None\n",
    "    \n",
    "    def fit(self, x, t):\n",
    "        \"\"\"partitions the data hierarchically by splitting it into two sets and\n",
    "        then recursively fitting a tree on each of these subsets\"\"\"\n",
    "        \n",
    "        assert x.ndim == 1, \"This tree can only fit 1 dimensional regression problems\"\n",
    "        assert x.shape == t.shape, \"x and t must have the same number of entries\"\n",
    "\n",
    "        # Insert code below!\n",
    "        \n",
    "        # make sure that the x values are in increasing order\n",
    "\n",
    "        # check if further splitting is allowed\n",
    "\n",
    "        # compute the split\n",
    "        \n",
    "        # create two subtrees and call their fit method\n",
    "\n",
    "        # store subtrees\n",
    "        self.subtrees = [l_subtree, r_subtree]\n",
    "        \n",
    "    def predict(self, x):\n",
    "        \"\"\"Returns the prediction of the tree for a given input x.\"\"\"\n",
    "        #Insert code below!\n",
    "        \n",
    "        # check if this is a leaf and return its prediction if so\n",
    "\n",
    "        # if not, return the prediction of the appropriate child (check split criterion)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building our first regression Tree\n",
    "\n",
    "We can now start to grow our first regression trees. We look at a simple regression problem: we draw sample from a one-dimensional function and add some observational noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def target_function(x, noise_level=0):\n",
    "    \"\"\"convenience function to easily exchange the function to be fitted\"\"\"\n",
    "    return(np.sinc(x) + noise_level*np.random.randn(len(x)))\n",
    "\n",
    "# defines the x-interval for all following plots\n",
    "x_interval = [0,4]\n",
    "\n",
    "\n",
    "def generate_data (N_points, noise = 0):\n",
    "    \"\"\"automatically evaluates the target function on equally spaced points and adds some gaussian noise\"\"\"\n",
    "    x_train = np.linspace(x_interval[0],x_interval[1],N_points)\n",
    "    y_train = target_function(x_train, noise)\n",
    "    return(x_train, y_train)\n",
    "\n",
    "\n",
    "N_points=64\n",
    "noise = 0.05\n",
    "\n",
    "\n",
    "x_train, y_train = generate_data(N_points, noise)\n",
    "x_dense = np.linspace(x_interval[0], x_interval[1], 512)\n",
    "plt.plot(x_dense, target_function(x_dense,0), 'r',label=('f(x)'))\n",
    "plt.scatter(x_train, y_train, c='black',label='observations')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first see how the decision tree fits these datapoints when allowed to split into single observations (min_leaf=1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tree = Tree(max_depth=N_points)\n",
    "tree.fit(x_train, y_train)\n",
    "plt.plot(x_dense, np.sinc(x_dense), c='r', label=('f(x)'))\n",
    "plt.scatter(x_train, y_train, c='black', label='observations')\n",
    "\n",
    "ys = [tree.predict(x) for x in x_dense]\n",
    "plt.step(x_dense,ys, c='b',label='prediction')\n",
    "plt.legend(loc=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not bad, but clearly overfitting to observational noise. You can make that more apparent by increasing the noise. Go back and adjust noise to, say, 0.1! Now the predictions are obviously bad. Keeping multiple data points in a leaf can improve the robustness (decrease variance):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train, y_train = generate_data(64, 0.1)\n",
    "\n",
    "for min_leaf in np.logspace(0,np.log2(N_points/4),5, base=2):\n",
    "    tree = Tree(max_depth=N_points, min_leaf = int(min_leaf))\n",
    "    tree.fit(x_train,y_train)\n",
    "    ys = [tree.predict(x) for x in x_dense]\n",
    "    plt.scatter(x_train, y_train, c='black', label='observations')\n",
    "    plt.plot(x_dense, np.sinc(x_dense), label=('f(x)'))\n",
    "    plt.step(x_dense,ys,label='prediction')\n",
    "    plt.legend(loc=1)\n",
    "    plt.title('min_leaf = %d'%int(min_leaf))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how 'pooling' observations in the leaves can increase the accuracy of the prediction.\n",
    "\n",
    "Modify the noise again, and observe how the 'optimal' value of min_leaf will change. Can you see a trend?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Trees via Bagging to a Random Forest\n",
    "\n",
    "Now that you have a working regression tree, it is time to study how an ensemble of them performs. For that, you have to fill in some code in the RandomForest class below. It follows the same interface of fit and predict of the Tree class above (and any scikit-learn algorithm).\n",
    "\n",
    "In the fit method, you have to draw a bootstrap sample (with replacement) of size 'datapoints_per_tree' for each tree in the ensemple. In the predict method, you compute the mean of the individual tree predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RandomForest(object):\n",
    "    \"\"\"A simple Random Forest\"\"\"\n",
    "    def __init__ (self, num_trees = 64, min_leaf=1, max_depth = 32, datapoints_per_tree = 16):\n",
    "        # initialize hyperparameters\n",
    "        self.num_trees = num_trees\n",
    "        self.min_leaf = min_leaf\n",
    "        self.max_depth = max_depth\n",
    "        self.datapoints_per_tree = datapoints_per_tree\n",
    "        \n",
    "        # store the individual trees in this list\n",
    "        self.trees = []\n",
    "    \n",
    "    def fit(self, x, t):\n",
    "        \"\"\" takes x and t and fits the specified number of trees to bootstrap samples\"\"\"\n",
    "        self.forest=[]\n",
    "        # Insert code below!\n",
    "            \n",
    "    def predict(self, x):\n",
    "        # Insert code below!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing the next cell will fit a forest to the data and plot the predictions of each tree separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, y_train = generate_data(64, 0.05)\n",
    "rf = RandomForest(num_trees = 16,min_leaf = 4, datapoints_per_tree = 48)\n",
    "rf.fit(x_train, y_train)\n",
    "for i,t in enumerate(rf.trees):\n",
    "    ys = [t.predict(x) for x in x_dense]\n",
    "    plt.step(x_dense,ys, label='prediction %d'%i)\n",
    "    plt.title('individual predictions of the %d trees'%int(min_leaf))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When pooling all the predictions into a single one, things start to look a lot smoother:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(x_dense, target_function(x_dense,0), 'r',label=('f(x)'))\n",
    "plt.scatter(x_train, y_train, label='observations')\n",
    "y_forest = [rf.predict(x) for x in x_dense]\n",
    "plt.plot(x_dense, y_forest, label='prediction')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to adjust the parameters num_trees, min_leaf, data_points_per_tree below to achieve a good overall prediction. Note how each of these parameters influences the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForest(num_trees = 4,min_leaf = 12, datapoints_per_tree = 64)\n",
    "rf.fit(x_train, y_train)\n",
    "plt.plot(x_dense, target_function(x_dense,0), 'r',label=('f(x)'))\n",
    "plt.scatter(x_train, y_train, label='observations')\n",
    "y_forest = [rf.predict(x) for x in x_dense]\n",
    "plt.plot(x_dense, y_forest, label='prediction')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Bonus(optional)\n",
    "At this point, you should have gained some familiarity with simple regression forests. If you are interested, here are some suggestions of what might interesting to try:\n",
    "1. If you haven't implement a randomized splitting point (not just the midpoint) in the 'split' function.\n",
    "2. In the lecture, different 'weak learners' were mentioned. So far, your implementation only predicts a contstant in each leaf. You could implement a simple linear model (scipy.stats.linregress is a good starting point) in the leaves. Note, this would also require to modify the 'split' and the 'squared_loss' function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
