{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different ensemble methods for classification trees\n",
    "\n",
    "In this notebook, you will use scikit-learn's trees and ensemble methods to compare different 'forests' and their performance on some toy datasets. In particular, you will\n",
    "\n",
    "1. train single Trees (deterministic and random ones, both greedy and random spilts) on the full data set, and compare performances.\n",
    "2. train multiple of them using bootstrapped samples of the data. compare performance vs. number of trees used.\n",
    "3. visualize some of the classifiers in 2d.\n",
    "4. train vs out ouf bag vs. test error vs. crossvalidation error.\n",
    "\n",
    "\n",
    "As usual, some setup first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.tree as sk_tree\n",
    "import sklearn.ensemble as sk_ensemble\n",
    "import sklearn.datasets as sk_data\n",
    "import sklearn.cross_validation as sk_cv\n",
    "import warnings\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set is a very simple example from scikit_learns datasets submodule. It's two dimensional, so you can visualize the classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X, Y = sk_data.make_moons(2000, noise=0.2)\n",
    "# an alternative data sets with more classes, if you feel fancy\n",
    "#X,Y = sk_data.make_blobs(n_samples=1000, n_features=2, centers=4, center_box=(-4,4))\n",
    "\n",
    "num_classes = len(np.unique(Y))\n",
    "cms = plt.cm.Paired\n",
    "\n",
    "for c in range(num_classes):\n",
    "    idx = (Y == c)\n",
    "    plt.scatter(X[idx, 0], X[idx,1], c = cms(c/num_classes), s=100)\n",
    "plt.show()\n",
    "\n",
    "h=0.02\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are two convenience functions that let you plot the 'decision boundaries' of a classifiers. The first one will plot all points that would be classified the same in the same color. The second one shows a more complex picture. It overlays all classes with the alpha value determined by the fraction of the samples in a particular leaf. That means, it illustrates the confidence of a tree.\n",
    "Feel free to use any of them for the remainder of the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classification_plot_max(trained_classifier, title):\n",
    "    fig = plt.figure()\n",
    "    ax = plt.subplot()\n",
    "    probs = trained_classifier.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
    "    \n",
    "    for c in range(num_classes):\n",
    "        tmp = np.array((np.argmax(probs, axis=1) == c).reshape(xx.shape), dtype=np.float)\n",
    "        tmp = cms(tmp)\n",
    "        ax.imshow(tmp, origin='lower', extent=[np.min(xx), np.max(xx), np.min(yy), np.max(yy)])\n",
    "        idx = (Y == c)\n",
    "        ax.scatter(X[idx, 0], X[idx,1], c = cms(c/num_classes), s=10, alpha=0.8)\n",
    "        if title is not None:\n",
    "            ax.set_title(title)\n",
    "    return(fig)\n",
    "\n",
    "\n",
    "def classification_plot_alpha(trained_classifier, title):\n",
    "    fig = plt.figure()\n",
    "    ax = plt.subplot()\n",
    "    for c in range(num_classes):\n",
    "        alphas = trained_classifier.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,c].reshape(xx.shape)\n",
    "        tmp = cms(c/num_classes*np.ones_like(xx))\n",
    "        tmp[:,:,3] = alphas\n",
    "        ax.imshow(tmp, origin='lower', extent=[np.min(xx), np.max(xx), np.min(yy), np.max(yy)])\n",
    "        idx = (Y == c)\n",
    "        ax.scatter(X[idx, 0], X[idx,1], c = cms(c/num_classes), s=10, alpha=0.8)\n",
    "        if title is not None:\n",
    "            ax.set_title(title)\n",
    "    return(fig)\n",
    "\n",
    "classification_plot = classification_plot_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some simple tree examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "classifiers = [(\"Decision Tree of depth  2\", sk_tree.DecisionTreeClassifier(max_depth=2)),\n",
    "               (\"Decision Tree of depth  4\", sk_tree.DecisionTreeClassifier(max_depth=4)),\n",
    "               (\"Decision Tree of depth  8\", sk_tree.DecisionTreeClassifier(max_depth=8)),\n",
    "               (\"Decision Tree of depth 16\", sk_tree.DecisionTreeClassifier(max_depth=16)),\n",
    "               (\"Extra Tree of depth  2\", sk_tree.ExtraTreeClassifier(max_depth=2)),\n",
    "               (\"Extra Tree of depth  4\", sk_tree.ExtraTreeClassifier(max_depth=4)),\n",
    "               (\"Extra Tree of depth  8\", sk_tree.ExtraTreeClassifier(max_depth=8)),\n",
    "               (\"Extra Tree of depth 16\", sk_tree.ExtraTreeClassifier(max_depth=16))\n",
    "            ]\n",
    "\n",
    "for name, classifier in classifiers:\n",
    "    classifier.fit(X,Y)\n",
    "    classification_plot(classifier, name)\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's your turn to draw similar pictures for the following classifiers:\n",
    "1. bagged classification trees\n",
    "2. random forests\n",
    "3. extra trees\n",
    "\n",
    "All are accessible in scikit learn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Insert code below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the ensemble methods generalize and how the out of bag error estimates the validation error. To make things a bit more interesting, we shall use a higher dimensional problem with more classes and more features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X,Y = sk_data.make_classification(10000, n_features=20, n_classes = 4, n_informative = 4)\n",
    "X_train, X_test, Y_train, Y_test = sk_cv.train_test_split(X, Y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "num_trees = [2*i for i in range(1,33)]\n",
    "accuracies = []\n",
    "for nt in num_trees:\n",
    "    # Sometimes not every input point was 'out of bag' and scikit learn raises a warning when computing the OOB-score.\n",
    "    # This just supresses this warning to unclutter the notebook.\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        rf = sk_ensemble.RandomForestClassifier(n_estimators=nt, bootstrap=True, oob_score=True, random_state=42)\n",
    "        rf.fit(X_train, Y_train)\n",
    "    accuracies.append((rf.score(X_train, Y_train), rf.score(X_test, Y_test), rf.oob_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracies = np.array(accuracies)\n",
    "plt.plot(num_trees, accuracies[:,0], label='train accuracy', linewidth=2)\n",
    "plt.plot(num_trees, accuracies[:,1], label='test accuracy')\n",
    "plt.plot(num_trees, accuracies[:,2], label='OOB accuracy')\n",
    "plt.ylim([0.5,1.05])\n",
    "plt.title(\"Train, test and out-of-bag error of a Random Forest\")\n",
    "plt.xlabel(\"Number of trees in the forest\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.grid('on','both')\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "\n",
    "What are advantages/disadvantages of using the out-of-bag error rather than the 'traditional' train/test split?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply this to a 'real' dataset and investigate some more. In the cell below, the iris data set is loaded. Add code to compute:\n",
    "1. the training error\n",
    "2. the test error\n",
    "3. the out of bag error\n",
    "4. a crossvalidation estimate of the error using 4-fold CV\n",
    "\n",
    "for different numbers of trees.\n",
    "\n",
    "Create plots similar to the one above for a random forests and extra trees. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We use the Iris data from the first exercise sheet\n",
    "X_train = np.loadtxt('iris_train.data', delimiter=' ', dtype=float)\n",
    "Y_train = np.loadtxt('iris_train.labels', dtype=int)\n",
    "X_test = np.loadtxt('iris_test.data', delimiter=' ', dtype=float)\n",
    "Y_test = np.loadtxt('iris_test.labels', dtype=int)\n",
    "\n",
    "# Insert code below!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
