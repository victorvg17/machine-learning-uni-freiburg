{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Boosting Decision Stumps\n",
    "\n",
    "In this notebook, you will use [scikit-learn's decision trees](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) \n",
    "and implement the AdaBoost algorithm for binary classification.\n",
    "\n",
    "The exercise is mostly based on the lecture and the following book:\n",
    "\n",
    "T. Hastie, R. Tibshirani, and J. Friedman: [*The Elements for Statistical Learning*](http://statweb.stanford.edu/~tibs/ElemStatLearn/), 2001\n",
    "\n",
    "As usual, some setup first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "import sklearn.tree\n",
    "import sklearn.base\n",
    "import sklearn.metrics\n",
    "import sklearn.ensemble\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set is an example from [scikit-learn's datasets submodule](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_hastie_10_2.html) which was introduced by [Hastie, Tibshirani and Friedman]. It has 10 features, which are standard independent Gaussian. The deterministic target Y is defined with:\n",
    "\n",
    "```   y[i] = 1 if np.sum(X[i] ** 2) > 9.34 else -1```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a binary classification dataset\n",
    "def create_dataset():\n",
    "    num_samples=12000\n",
    "    n_train = 2000\n",
    "    \n",
    "    X, y = sklearn.datasets.make_hastie_10_2(n_samples=num_samples, \n",
    "                                             random_state=1)\n",
    "    X_train = X[:n_train]\n",
    "    y_train = y[:n_train]\n",
    "    X_test = X[n_train:]\n",
    "    y_test = y[n_train:]\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBooster(object):\n",
    "    \n",
    "    def __init__(self, weak_learner, iterations):\n",
    "        '''\n",
    "        weak_learner: sklearn classifier - G_m\n",
    "        iterations: number of iteration to train - M\n",
    "        '''\n",
    "        self.weak_learner = weak_learner\n",
    "        self.iterations = iterations\n",
    "        \n",
    "        # 1. Initialize some more lists\n",
    "        self.classifiers = list() # trained classifiers G_m\n",
    "        self.alphas = list()      # classifier weights       \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # YOUR TURN\n",
    "        # 1. initialize the observation weights\n",
    "        \n",
    "        # 2. For m=1 to M\n",
    "        # hint: Use sklearn.base.clone method to copy the classifier \n",
    "        #       in each iteration\n",
    "        # (a) Fit a classifier using the weights\n",
    "        #     hint: Use 'sample_weights' in the fit() method of the weak_learner\n",
    "        #     See also: http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "        # (b) Compute error\n",
    "        # (c) Compute classifier weight\n",
    "        # (d) Update observation weights\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Compute predictions by looping over all classifiers \n",
    "        # and adding weighted predictions. \n",
    "        # The actual output should be -1 if y_i <= 0 and 1 otherwise\n",
    "    \n",
    "        # YOUR TURN\n",
    "        pred = np.zeros(X.shape[0])\n",
    "        \n",
    "        return pred\n",
    "    \n",
    "    def staged_predict(self, X):\n",
    "        # returns predictions for X for all iterations \n",
    "        # this is only for convenience to simplify computing\n",
    "        # train/test error for all iterations\n",
    "\n",
    "        # YOUR TURN\n",
    "        staged_predictions = []\n",
    "            \n",
    "        return staged_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your implementation\n",
    "\n",
    "Now, you have a working implementation of the AdaBoost algorithm. To test your algorithm for correctness you can compare to [sklearns AdaBoost implementation](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html) which should yield the same output as your implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell to test whether your implementation is correct\n",
    "\n",
    "weak_learner = sklearn.tree.DecisionTreeClassifier(max_depth=1)\n",
    "adaboost = AdaBooster(weak_learner=weak_learner, iterations=400)\n",
    "sk_adaboost = sklearn.ensemble.AdaBoostClassifier(weak_learner, \n",
    "                                                  algorithm=\"SAMME\", \n",
    "                                                  n_estimators=400)\n",
    "\n",
    "# Get data\n",
    "X_train, y_train, X_test, y_test = create_dataset()\n",
    "\n",
    "# Fit model\n",
    "adaboost.fit(X_train, y_train)\n",
    "sk_adaboost.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "pred = adaboost.predict(X_test)\n",
    "pred_sk = sk_adaboost.predict(X_test)\n",
    "\n",
    "print(\"Difference: %d\" % np.sum([pred != pred_sk]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment\n",
    "\n",
    "Next, we will experiment with AdaBoost's hyperparameters. Here are some suggestions to try:\n",
    "\n",
    " * Investigate the difference between performance on trainset and testset.\n",
    " * Add some noise on the (training) target and repeat.\n",
    " * Investigate the effect of the tree depth on the performance on trainset and testset.\n",
    " \n",
    "Before you investigate the model it will be helpful to prepare a function that generates a plot showing\n",
    "test/train performance over number of iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_test_error(model, X_train, y_train, X_test, y_test):\n",
    "    ''' Shows a plot with train/test error over iterations\n",
    "\n",
    "    model (AdaBooster): trained AdaBoost algorithm\n",
    "    X_train (array): [#samples, #features]\n",
    "    y_train (array): [#samples, ]\n",
    "    X_test (array): [#samples, #features]\n",
    "    y_test (array): [#samples, ]\n",
    "    '''\n",
    "    # Loop over all predictions and compute accuracy\n",
    "    y_pred_train = model.staged_predict(X_train)\n",
    "    training_errors = [sklearn.metrics.accuracy_score(y_train, y_pred) \n",
    "                       for y_pred in y_pred_train]\n",
    "    \n",
    "    y_pred_test = model.staged_predict(X_test)\n",
    "    test_errors = [sklearn.metrics.accuracy_score(y_test, y_pred) \n",
    "                   for y_pred in y_pred_test]\n",
    "    \n",
    "    # Plot results\n",
    "    x_values = np.arange(1, len(training_errors) + 1)\n",
    "\n",
    "    plt.plot(x_values, training_errors, label=\"training\")\n",
    "    plt.plot(x_values, test_errors, label=\"test\")\n",
    "    plt.xlabel(\"iterations\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.ylim([0, 1])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TURN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
