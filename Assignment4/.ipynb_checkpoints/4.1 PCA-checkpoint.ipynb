{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4.1: Principal Component Analysis\n",
    "\n",
    "In the current exercise, we will explore some properties, as well as limits, of Principal Component Analysis.\n",
    "\n",
    "### Exploring data and plotting the Principal Components\n",
    "__Task__: Given the two-dimensional zero-mean unlabeled dataset, compute and plot the principal components (PCs) together with the scatter plot of the data. Use the eigenvalue decomposition of the covariance matrix of the data to find the PC.\n",
    "\n",
    "__Hint 1__: The directions of the PCs are given by the eigenvectors of the sample covariance matrix.\n",
    "\n",
    "__Hint 2__: Scale length of the PCs using the square root of the eigenvalues of sample covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 2)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "X = np.loadtxt('data/PCA_1.data')\n",
    "N_samples = X.shape[0]\n",
    "\n",
    "# compute covariance matrix\n",
    "X.shape\n",
    "\n",
    "# compute eigenvalue decomposition and sort eigenvalues if necessary\n",
    "\n",
    "# plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Important note__ : Computational complexity of calculating the eigenvalue decomposition grows cubically with the number of features. Therefore, iterative approaches for computing the PCs are prefered when dealing with high dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How outliers and scaling of the data affect PCA\n",
    "\n",
    "The following dataset is similar to the one analyzed before, however it contains outliers. Explore the PCs again in the presence of outliers.\n",
    "\n",
    "__Task:__ Compute and plot the PCs together with the scatter plot of the data.\n",
    "\n",
    "__Q4.1.1 Why and how is PCA prone to fail in the presence of outliers?__\n",
    "\n",
    "__Q4.1.2 How does the scaling of the features affect PCA? For example: Using centimeters instead of meters.__\n",
    "\n",
    "__Hint:__ PCA uses variance of the data as criterion to find the PCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "X = np.loadtxt('data/PCA_2.data')\n",
    "N_samples = X.shape[0]\n",
    "\n",
    "# compute covariance matrix\n",
    "sample_cov = np.cov(X,rowvar=0)\n",
    "\n",
    "# compute eigenvalue decomposition of covariance matrix. Order eigenvalues if necessary\n",
    "\n",
    "# plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality reduction with PCA as preprocessing for classification problems\n",
    "\n",
    "In the following exercise, we will see how PCA can be used to reduce the dimensionality of a dataset as a pre-processing stage for a classification task.\n",
    "\n",
    "__Task 3__: For the first dataset, plot the PCs together with the scatter plot of the labelled data. Set the color of each of the points according to its class.\n",
    "\n",
    "__Task 4__: Project the data onto each of the PCs and plot the projection. \n",
    "\n",
    "__Q4.1.3 Which of the PCs is more adequate to perform dimensionality reduction in the labelled dataset? Why?__ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "X = np.loadtxt('data/PCA_3.data')\n",
    "y = np.loadtxt('data/PCA_3.labels')\n",
    "N_samples = X.shape[0]\n",
    "\n",
    "# compute covariance matrix\n",
    "\n",
    "# compute eigenvalue decomposition of covariance matrix. Order eigenvalues if necessary\n",
    "\n",
    "# plot original data\n",
    "\n",
    "# plot data projection (first eigenvector)\n",
    "\n",
    "# plot data projection (second eigenvector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 5__ Repeat tasks 3 and 4 for the following dataset.\n",
    "\n",
    "__Q4.1.4 What is the most relevant PC in this case, given that the goal is to perform dimensionality reduction for a classification task?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "X = np.loadtxt('data/PCA_4.data')\n",
    "y = np.loadtxt('data/PCA_4.labels')\n",
    "N_samples = X.shape[0]\n",
    "\n",
    "# compute covariance matrix\n",
    "\n",
    "# compute eigenvalue decomposition of covariance matrix. Order eigenvalues if necessary\n",
    "\n",
    "# plot original data\n",
    "\n",
    "# plot data projection (first eigenvector)\n",
    "\n",
    "# plot data projection (second eigenvector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA for dimensionality reduction: Selecting the number of PCs in a high dimensional dataset\n",
    "In this section, we will explore the criteria that might be used to select the number of components for a dimensionality reduction task using PCA.\n",
    "\n",
    "__Task 6__: Plot the eigenvalue spectrum of the following dataset. \n",
    "\n",
    "__Q4.1.5: By visually inspecting the eigenvalue spectrum of the covariance matrix, how many components do you think are necessary to achieve a good representation of the variance of the data?__\n",
    "\n",
    "__Hint:__ The magnitude of the eigenvalues represents the contribution of the corresponding PCs to the variance of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "X = np.loadtxt('data/PCA_5.data')\n",
    "N_samples = X.shape[0]\n",
    "\n",
    "# compute covariance matrix\n",
    "\n",
    "# compute eigenvalue decomposition of covariance matrix. Order eigenvalues if necessary\n",
    "\n",
    "# plot eigenvalue spectrum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually inspecting the eigenvalue spectrum of the covariance matrix do not always provide a clear criterion for the selection of the number of relevant components. If visualization does not deliver a clear cut-off point, an alternative criterion might be used: Typically, the number of PCs is selected so as to accumulate an __arbitrary__ amount of the variance, e.g. 90%, 95% or 99% of accumulated variance.\n",
    "\n",
    "__Q4.1.6: Using the cumulative variance criterion, how many components should be selected for performing dimensionality reduction in the following dataset?__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "X = np.loadtxt('data/PCA_6.data')\n",
    "N_samples = X.shape[0]\n",
    "\n",
    "#compute covariance matrix\n",
    "\n",
    "# compute eigenvalue decomposition of covariance matrix. Order eigenvalues if necessary\n",
    "\n",
    "# plot eigenvalue spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
