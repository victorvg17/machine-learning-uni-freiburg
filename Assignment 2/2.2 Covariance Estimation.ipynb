{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covariance estimation from data sets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Inspect 2-dimensional data set\n",
    " \n",
    "__Tasks:__ \n",
    " 1. Inspect how the 2-dimensional toy data are constructed. \n",
    " 2. Visually inspect the data using a scatterplot (plt.scatter(...)) and a histogram (plt.hist(...)). Use different configurations of the covariance and plot them all to the same figure. Therefore, please review the properties of a covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assign the \"true\" mean and covariance of your toy dataset\n",
    "mean = [5, 0]\n",
    "cov = [[2, 1], [1, 2]]  # diagonal covariance\n",
    "N_samples = 300\n",
    "# Draw samples from your dataset\n",
    "X = np.random.multivariate_normal(mean, cov, N_samples)\n",
    "\n",
    "# TO DO: Scatter plot. Try different colors for different covariance matrices (use c=' ' parameter)\n",
    "\n",
    "\n",
    "# TO DO: Histogram in both dimensions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance estimator for N-dimensional data\n",
    "\n",
    "The function sample_generator() generates toy data for a given number of samples and features and draws its samples from a D-dimensional normal distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sample_generator(N_samples, D_features):\n",
    "    '''returns N_samples from a p=D_features variate normal distribution\n",
    "    Input: N_samples, D_features\n",
    "    Output: data matrix X (size: NxD)\n",
    "           true covariance matrix true_cov (size: ?x?)'''\n",
    "\n",
    "    # Construct a positive semi-definite covariance matrix\n",
    "    tmp = np.random.randn(D_features,D_features)\n",
    "    true_cov = np.dot(tmp,tmp.transpose()) \n",
    "\n",
    "    mean = [0]*D_features\n",
    "    # get samples from multivariate normal distribution\n",
    "    X = np.random.multivariate_normal(mean, true_cov, N_samples)\n",
    "    return X, true_cov\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Tasks:__ \n",
    "  1. Implement a function estimate_covariance() that calculates the covariance matrix for a given data set $X$ of size $(N_{samples} \\times D_{features})$. You can test your function with a data set returned from the function sample_generator(). \n",
    "  2. Compare your resulting covariance matrix with the corresponding built-in-function in the \"Numpy\" package. Please carefully check the documentation of the function for the definition of the input. You can visualize covariance matrices e.g. with the function plt.imshow(...) from the \"Matplotlib\" package. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_covariance(X):\n",
    "    '''estimate the covariance matrix\n",
    "    Input: data matrix X (size: NxD) \n",
    "    Output: covariance matrix cov (size: ?x?)'''\n",
    "    \n",
    "    \n",
    "    \n",
    "    return cov\n",
    "\n",
    "D_features = 100\n",
    "N_samples = 200\n",
    "X, true_cov = sample_generator(N_samples, D_features)\n",
    "\n",
    "# estimate covariance using your implementation\n",
    "cov = estimate_covariance(X)\n",
    "\n",
    "# for comparison: estimate covariance using the numpy-function \n",
    "\n",
    "\n",
    "# plot the covariance matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Visualizing the eigenvectors of the covariance matrix\n",
    "\n",
    "In this (optional) task, we want to shed light on the relationship between the eigenvalues / eigenvectors of the covariance matrix and the shape of the normal distribution in a 2D scenario. \n",
    "\n",
    "__Tasks__: 1.) Visualize a randomly sampled normal distribution (e.g. by using the seaborn function sns.jointplot()). The seaborn package is a matplotlib-based visualization package. You can install it by typing the following command in the terminal: __\"sudo pip3 install seaborn\"__. If you run the script and you do not see the data, also install the cairo backend with __\"sudo pip3 install cairocffi\"__.)\n",
    "\n",
    "2.) Calculate eigenvalues and eigenvectors of the true covariance matrix.\n",
    "\n",
    "3.) Draw the estimated eigenvectors multiplied by the square root of the corresponding eigenvalues in the same plot.\n",
    "\n",
    "#### Q2.2.1 (Bonus) What do you observe regarding the size of the eigenvalues and the shape of the covariance matrix?\n",
    "\n",
    "#### Q2.2.2 (Bonus) If the eigenvalues are over -  or underestimated, what does this mean for the estimated shape of the normal distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1235)\n",
    "D_features = 2\n",
    "N_samples = 500\n",
    "X, true_cov = sample_generator(N_samples, D_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvalue Spectrum for various sample sizes\n",
    "\n",
    "__Tasks:__ For a fixed number of features $D_{features} = 100$, vary the number of samples $N_{samples}$ and implement the following steps:\n",
    " 1. Estimate the covariance matrix from the generated samples X\n",
    " 2. Calculate the eigenvalues of the covariance\n",
    " 3. Sort the eigenvalues in descending order\n",
    " 4. Plot the eigenvalues as a function of their order/rank (called eigenvalue spectrum)\n",
    "\n",
    "This procedure should be repeated for the given sample sizes $N_{samples}=[20,50,100,300,1000]$ and all eigenvalue spectra shown in one figure. For comparison, repeat the steps (2)-(4) also for the true covariance matrix and add this eigenvalue spectrum to the created figure. Here is an example obtained for a given ground truth covariance matrix and one calculated using 120 samples from the same distribution\n",
    "\n",
    "![title](data/eigenvalue_spectrum.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_features = 100\n",
    "N_samples = [20,50,100,300,1000]\n",
    "\n",
    "for index, N_samples in enumerate(N_samples):\n",
    "    \n",
    "    # generate data set with N_samples\n",
    "    X, true_cov = sample_generator(N_samples, D_features)\n",
    "\n",
    "    # estimate covariance from the data X \n",
    "\n",
    "    \n",
    "    # calculate eigenvectors of estimated covariance \n",
    "\n",
    "    \n",
    "    # plot the eigenvalue-spectrum for each sample configuration\n",
    "    \n",
    "\n",
    "    \n",
    "# Calculate and plot eigenvalue spectrum of true covariance matrix\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2.2.3) What do you observe for the largest and smallest estimates of the eigenvalues?\n",
    "\n",
    "#### Q2.2.4) What do you observe for an increasing number of N_samples?\n",
    "\n",
    "#### Q2.2.5) Which condition for the N_samples in relation to D_features is necessary to allow a training of the LDA?\n",
    "\n",
    "#### Q2.2.6) How can you manipulate your DATA to avoid this problem, in case of N_samples < D_features?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus:  Improving covariance estimation by means of regularization\n",
    "\n",
    "You may have observed a systematic error in the eigenvalues estimates, even for N_samples > D_features. \n",
    "\n",
    "#### Q2.2.7) (Bonus) Investigate a way to compensate for this systematic error such that the estimated spectrum approximates the true spectrum more closely. __Hint:__ Regularized estimation of covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y_pred, y_true):\n",
    "    ''' returns the mean squared error \n",
    "    Input:estimated eigenvalue spectrum y_pred of size D \n",
    "          true eigenvalue spectrum y_true of size D\n",
    "    Output: mean squared error ms_error'''\n",
    "    \n",
    "    y_true = np.sort(np.real(y_true))\n",
    "    y_pred = np.sort(np.real(y_pred))\n",
    "    ms_error = np.linalg.norm(y_pred-y_true)\n",
    "    \n",
    "    return ms_error\n",
    "\n",
    "def regularized_covariance(X):\n",
    "    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! #\n",
    "    # Implement here your improved estimation of the covariance matrix #\n",
    "    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! #\n",
    "    improved_cov = np.cov(X.T)\n",
    "    return improved_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this seed ensures comparable results when repeating the calculation. Please keep this unchanged!!!\n",
    "np.random.seed(1337)\n",
    "\n",
    "# please keep this N_samples unchanged!\n",
    "N_samples = 120\n",
    "X, true_cov = sample_generator(N_samples, D_features)\n",
    "\n",
    "# true eigenvalue spectrum (sorted)\n",
    "true_w, true_v = np.linalg.eig(true_cov)\n",
    "true_w = np.array(sorted(true_w, reverse=True))\n",
    "\n",
    "# calculate eigenvectors of estimated covariance \n",
    "emp_cov = np.cov(X.T)\n",
    "emp_w, emp_v = np.linalg.eig(emp_cov)\n",
    "emp_w = np.array(sorted(np.real(emp_w), reverse=True))\n",
    "\n",
    "# Your approach\n",
    "improved_cov = regularized_covariance(X)\n",
    "improved_w, emp_v = np.linalg.eig(improved_cov)\n",
    "improved_w = np.array(sorted(np.real(improved_w), reverse=True))\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(emp_w, label='empirical cov using %s samples' % N_samples)\n",
    "plt.plot(true_w, label='true cov')\n",
    "plt.xlabel('rank #', fontsize=16)\n",
    "plt.ylabel('eigenvalue', fontsize=16)\n",
    "plt.xlim(0,100)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# report on the mean squared error between true covariance and your covariance estimation\n",
    "# as a baseline, you need to improve upon the mean squared error between emp_w and true_w\n",
    "error = mean_squared_error(emp_w,true_w)\n",
    "print('The mean squared error vs no regularized is %.1f' %error)\n",
    "\n",
    "error = mean_squared_error(improved_w,true_w)\n",
    "print('The mean squared error vs regularized is %.1f' %error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": null
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
